{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b52028e-7227-43ac-b6eb-5ad7bd0707bb",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01848d-2057-4b71-ae05-1665552c2fab",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for supervised learning tasks, particularly for regression problems. It is a type of boosting algorithm that iteratively improves a weak regression model by minimizing the loss function. The algorithm works by combining multiple weak models (usually decision trees) to create a single, strong model.\n",
    "\n",
    "The main idea behind Gradient Boosting Regression is to train each subsequent model to focus on the errors made by the previous models. The algorithm achieves this by using the gradient descent optimization method to iteratively improve the model. In each iteration, a new model is trained to fit the negative gradient of the loss function with respect to the output of the previous model. The output of the new model is then added to the output of the previous models to update the predictions.\n",
    "\n",
    "The process is repeated until the loss function is minimized, or a specified number of iterations is reached. The final model is a weighted sum of the outputs of all the models, with the weights determined by the performance of each model on the training data.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex, non-linear relationships in the data, and to effectively handle high-dimensional datasets. It is a widely used technique in machine learning and is implemented in various popular libraries such as Scikit-learn, XGBoost, and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f273540-1754-4640-90a8-b9652809c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb9ad9e-3523-4e96-a641-62f3a71f84b3",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a3587-9cc1-4c23-bb69-68a9fb83624c",
   "metadata": {},
   "source": [
    "### Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f673b9b2-487c-4eaa-9198-0e265abd6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1000,n_features=4,\n",
    "                           n_informative=2,\n",
    "                          random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13b3703-a7eb-44a2-816c-dab9801a4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e71b5aad-6433-48ef-8fbf-d789745b35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bda5f0a-d13d-4a99-8c12-0d9ba438cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor=GradientBoostingRegressor()\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f15637a3-6239-41cb-aeb4-1e7aeff27345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 76.02512025437463\n",
      "R-squared : 0.9898239752946818\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "# calculate the mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('Mean squared error:', mse)\n",
    "print('R-squared :', r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f189d7-865a-4c46-90fd-f740777e00ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd7bc72-4437-4bcb-bfde-c78479f98d53",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e426-8cf2-4435-8718-11f467bb6d49",
   "metadata": {},
   "source": [
    "### Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d953155a-b380-48ae-a15f-4578661835ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45be4597-5f06-4df7-9256-7f50b0384e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'n_estimators': [100, 500, 1000],\n",
    "              'max_depth': [3, 5, 7]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7755e36f-4a9e-4028-ba58-161d95d4a167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.993 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.993 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.991 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.993 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.994 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=500;, score=0.995 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=500;, score=0.995 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=500;, score=0.993 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=500;, score=0.994 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=500;, score=0.995 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=1000;, score=0.995 total time=   1.4s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=1000;, score=0.995 total time=   1.4s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=1000;, score=0.993 total time=   1.5s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=1000;, score=0.994 total time=   1.5s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=1000;, score=0.995 total time=   1.5s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.991 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.992 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.989 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.993 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.993 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=500;, score=0.992 total time=   1.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=500;, score=0.992 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=500;, score=0.989 total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=500;, score=0.993 total time=   1.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=500;, score=0.994 total time=   1.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=1000;, score=0.991 total time=   2.1s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=1000;, score=0.992 total time=   2.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=1000;, score=0.989 total time=   2.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.1s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.986 total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.991 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.987 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.991 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.990 total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=7, n_estimators=500;, score=0.985 total time=   1.3s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=7, n_estimators=500;, score=0.991 total time=   1.3s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=7, n_estimators=500;, score=0.987 total time=   1.3s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=7, n_estimators=500;, score=0.990 total time=   1.3s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=7, n_estimators=500;, score=0.990 total time=   1.3s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=7, n_estimators=1000;, score=0.985 total time=   2.7s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=7, n_estimators=1000;, score=0.991 total time=   2.6s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=7, n_estimators=1000;, score=0.987 total time=   2.6s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=7, n_estimators=1000;, score=0.990 total time=   2.6s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=7, n_estimators=1000;, score=0.990 total time=   2.6s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.990 total time=   0.1s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.990 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.987 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.988 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.992 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=3, n_estimators=500;, score=0.994 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=3, n_estimators=500;, score=0.995 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=3, n_estimators=500;, score=0.993 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=3, n_estimators=500;, score=0.994 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=3, n_estimators=500;, score=0.995 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=3, n_estimators=1000;, score=0.995 total time=   1.4s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=3, n_estimators=1000;, score=0.996 total time=   1.4s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=3, n_estimators=1000;, score=0.993 total time=   1.5s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=3, n_estimators=1000;, score=0.994 total time=   1.4s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=3, n_estimators=1000;, score=0.996 total time=   1.4s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=100;, score=0.990 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=100;, score=0.993 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=100;, score=0.989 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=100;, score=0.993 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=100;, score=0.994 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=500;, score=0.992 total time=   1.0s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=500;, score=0.994 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=500;, score=0.990 total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=500;, score=0.994 total time=   1.0s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=500;, score=0.994 total time=   1.0s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=5, n_estimators=1000;, score=0.992 total time=   2.0s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=5, n_estimators=1000;, score=0.990 total time=   2.0s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=7, n_estimators=100;, score=0.986 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=7, n_estimators=100;, score=0.990 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=7, n_estimators=100;, score=0.986 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=7, n_estimators=100;, score=0.990 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=7, n_estimators=100;, score=0.991 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=7, n_estimators=500;, score=0.986 total time=   1.3s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=7, n_estimators=500;, score=0.990 total time=   1.3s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=7, n_estimators=500;, score=0.987 total time=   1.3s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=7, n_estimators=500;, score=0.991 total time=   1.3s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=7, n_estimators=500;, score=0.991 total time=   1.3s\n",
      "[CV 1/5] END learning_rate=0.05, max_depth=7, n_estimators=1000;, score=0.986 total time=   2.6s\n",
      "[CV 2/5] END learning_rate=0.05, max_depth=7, n_estimators=1000;, score=0.990 total time=   2.6s\n",
      "[CV 3/5] END learning_rate=0.05, max_depth=7, n_estimators=1000;, score=0.987 total time=   2.6s\n",
      "[CV 4/5] END learning_rate=0.05, max_depth=7, n_estimators=1000;, score=0.990 total time=   2.6s\n",
      "[CV 5/5] END learning_rate=0.05, max_depth=7, n_estimators=1000;, score=0.991 total time=   2.6s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.739 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.741 total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.747 total time=   0.1s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.752 total time=   0.1s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.761 total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=500;, score=0.989 total time=   0.7s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=500;, score=0.991 total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=500;, score=0.987 total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=500;, score=0.989 total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=500;, score=0.992 total time=   0.7s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=1000;, score=0.993 total time=   1.4s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=1000;, score=0.994 total time=   1.4s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=1000;, score=0.992 total time=   1.4s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=1000;, score=0.993 total time=   1.4s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=1000;, score=0.995 total time=   1.5s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.820 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.835 total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.828 total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.829 total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.827 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=500;, score=0.991 total time=   1.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=500;, score=0.993 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=500;, score=0.989 total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=500;, score=0.993 total time=   1.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=500;, score=0.994 total time=   1.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=1000;, score=0.992 total time=   2.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=1000;, score=0.990 total time=   2.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=1000;, score=0.994 total time=   2.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.840 total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.848 total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.835 total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.833 total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.847 total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=7, n_estimators=500;, score=0.986 total time=   1.2s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=7, n_estimators=500;, score=0.990 total time=   1.2s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=7, n_estimators=500;, score=0.986 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=7, n_estimators=500;, score=0.990 total time=   1.3s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=7, n_estimators=500;, score=0.991 total time=   1.3s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=7, n_estimators=1000;, score=0.986 total time=   2.5s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=7, n_estimators=1000;, score=0.991 total time=   2.5s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=7, n_estimators=1000;, score=0.987 total time=   2.5s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=7, n_estimators=1000;, score=0.990 total time=   2.5s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=7, n_estimators=1000;, score=0.991 total time=   2.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.05, 0.01],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [100, 500, 1000]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.05, 0.01],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [100, 500, 1000]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.1, 0.05, 0.01],\n",
       "                         'max_depth': [3, 5, 7],\n",
       "                         'n_estimators': [100, 500, 1000]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=5,verbose=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c3f5bb2-80e8-45a4-833f-abb3cc59b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "###  Print the best hyperparameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef6aa4-2442-4c6d-8065-65e9ad5a1b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd565b5a-15a7-4080-a421-c64d9a1c19ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.05, n_estimators=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.05, n_estimators=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, n_estimators=1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor1=GradientBoostingRegressor(learning_rate=0.05,max_depth=3,n_estimators=1000)\n",
    "regressor1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d572c667-12fa-4494-851e-a27c49cb42b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 76.02512025437463\n",
      "R-squared : 0.9898239752946818\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test data\n",
    "y_pred1 = regressor1.predict(X_test)\n",
    "# calculate the mean squared error and R-squared\n",
    "mse1 = mean_squared_error(y_test, y_pred1)\n",
    "r21 = r2_score(y_test, y_pred1)\n",
    "print('Mean squared error:', mse)\n",
    "print('R-squared :', r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c1e4e-d4b1-42db-bd21-08e5cde004d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252d2c24-0c05-4d62-805a-161ddf433245",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc3a97-b724-4da1-8df4-229a63e26247",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a model that performs only slightly better than random guessing. In the context of Gradient Boosting Regression, weak learners are typically decision trees with a shallow depth, few split points, and low complexity. These models are also referred to as \"stumps\" due to their small size.\n",
    "\n",
    "Weak learners are used in Gradient Boosting because they are easy to train and have low variance, which helps to prevent overfitting. The Gradient Boosting algorithm combines multiple weak learners to create a strong, high-performing model. The combination of multiple weak models results in a more complex and powerful model that can capture non-linear relationships in the data.\n",
    "\n",
    "The key to the success of Gradient Boosting is the iterative process of adding weak learners to the model, with each subsequent model trained to fit the errors made by the previous models. This iterative process allows the algorithm to focus on the most difficult examples, which leads to improved performance and accuracy.\n",
    "\n",
    "Overall, weak learners are an important component of the Gradient Boosting algorithm, as they provide the building blocks for creating a strong and powerful model that can effectively model complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdc94b-700f-4289-b5ab-1188c85ac32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d2e2e6-3093-4683-93ed-86a87e1af45a",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866bf58-fe9a-4e91-955e-bd3d8c0dbe75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans:\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to combine the predictions of multiple weak learners (e.g. decision trees) to create a single strong learner that can accurately predict the target variable.\n",
    "\n",
    "The algorithm works by initially fitting a simple model to the data, such as a decision tree with just a few splits. The model is then used to make predictions on the training data, and the errors (i.e. differences between the predicted and actual values) are calculated.\n",
    "\n",
    "The next step is to fit a new model to the errors from the previous model, such that the new model is optimized to correct the errors made by the previous model. This process is repeated, with each new model trained to correct the errors of the previous models.\n",
    "\n",
    "Each new model is added to the ensemble of models that have been trained so far, with the weights of each model determined based on its performance on the training data. The final prediction is a weighted sum of the predictions of all the models.\n",
    "\n",
    "The intuition behind the algorithm is that by focusing on the errors made by the previous models, the Gradient Boosting algorithm is able to iteratively improve the overall model, and create a strong learner that can accurately predict the target variable.\n",
    "\n",
    "Another key aspect of the Gradient Boosting algorithm is the use of gradient descent optimization to update the model in each iteration. This optimization technique helps to improve the performance of the algorithm, and to ensure that the models are trained efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0c999-d2e8-4dee-ad88-2c89f917baa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6500f33-b72e-4d30-9f01-94027dfec5f3",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200edf9a-70fc-4bee-a55b-5949679f25ab",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the ensemble and updating the predictions of the ensemble at each step.\n",
    "\n",
    "In each iteration of the algorithm, a new weak learner (e.g. decision tree) is fit to the residuals of the previous predictions. The residuals are calculated as the difference between the true target values and the current predictions of the ensemble. The new weak learner is trained to predict these residuals, which represent the errors made by the current ensemble of weak learners.\n",
    "\n",
    "Once the new weak learner is trained, its predictions are added to the predictions of the previous weak learners, and the ensemble is updated. The weight of the new weak learner is determined based on its ability to reduce the loss function (e.g. mean squared error) on the training data.\n",
    "\n",
    "This process is repeated for a fixed number of iterations or until the loss function no longer improves on a validation set. The final ensemble of weak learners is a weighted sum of the predictions of all the models, with the weights determined based on their performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e3fd9-f888-4964-b041-aa5dea3d5bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25d1cf6-a475-4f66-a84e-63190c7315a9",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5004b-fda3-4e6e-80f8-907a2b234e71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans:\n",
    "\n",
    "### The mathematical intuition behind the Gradient Boosting algorithm can be constructed through the following steps:\n",
    "1. Define the loss function: The first step is to define a loss function that measures the error between the predicted values and the actual values. For regression problems, the mean squared error (MSE) is a common choice for the loss function.\n",
    "2. Initialize the model: The next step is to initialize the model with a simple model, such as a decision tree with only one split.\n",
    "3. Make predictions: The model is used to make predictions on the training data.\n",
    "4. Calculate residuals: The residuals are calculated by subtracting the predicted values from the actual values.\n",
    "5. Fit a new model: A new model is fit to the residuals, with the aim of predicting the residuals more accurately than the previous model. This can be done by fitting another decision tree to the residuals.\n",
    "6. Update predictions: The predictions are updated by adding the predictions from the new model to the predictions from the previous model.\n",
    "7. Repeat: Steps 4-6 are repeated iteratively until a stopping criterion is met, such as reaching a maximum number of iterations or achieving a desired level of performance.\n",
    "8. Combine models: The final model is a weighted sum of the predictions from all the models, with the weights determined based on the performance of each model on the training data.\n",
    "9. Predict new values: The final model is used to make predictions on new data.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is that each new model is trained to correct the errors made by the previous models, resulting in a final model that is capable of accurately predicting the target variable. The use of gradient descent optimization helps to improve the efficiency of the algorithm and ensure that the models are trained effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39146f8e-a798-4edf-a9d6-b74f55b9c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
