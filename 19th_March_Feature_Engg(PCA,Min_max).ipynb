{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddb3b0c-fff1-450b-be18-3a70a4f9be30",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its  application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b083362-5d73-4d67-aee1-bd4f046c8339",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "Min-Max scaling is a data preprocessing technique that involves rescaling the range of a feature to a specific range, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original feature, X_min is the minimum value of the feature, X_max is the maximum value of the feature, and X_scaled is the rescaled feature.\n",
    "\n",
    "Min-Max scaling is commonly used in data preprocessing to ensure that all features are on the same scale. This can help to improve the performance of machine learning models that are sensitive to the scale of the features, such as k-nearest neighbors and neural networks.\n",
    "\n",
    "Here is an example of Min-Max scaling in action:\n",
    "\n",
    "Suppose we have a dataset with a feature that represents the age of individuals. The age feature has a minimum value of 18 and a maximum value of 80. We want to rescale the feature so that it has a range between 0 and 1.\n",
    "\n",
    "To do this, we can use the Min-Max scaling formula:\n",
    "\n",
    "age_scaled = (age - 18) / (80 - 18)\n",
    "\n",
    "For example, if an individual is 35 years old, the rescaled age would be:\n",
    "\n",
    "age_scaled = (35 - 18) / (80 - 18) = 0.297\n",
    "\n",
    "This means that the rescaled age value for this individual would be 0.297, which is between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93070ef-ae9e-46fe-89bf-fe63a541c3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef7e880a-cd84-4ca8-926f-b39dd29dcb7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?  Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a2d24-de8d-480f-aab6-17bb965e246e",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "The Unit Vector technique is a feature scaling technique that involves scaling the values of a feature so that they have a unit length. This is done by dividing each value in the feature by the L2 norm of the feature.\n",
    "\n",
    "The formula for scaling a feature using the Unit Vector technique is as follows:\n",
    "\n",
    "X_scaled = X / ||X||\n",
    "\n",
    "where X is the original feature, ||X|| is the L2 norm (Euclidean norm) of the feature, and X_scaled is the scaled feature.\n",
    "\n",
    "The main difference between the Unit Vector technique and Min-Max scaling is that the Unit Vector technique scales the values of a feature so that they have a unit length, while Min-Max scaling rescales the range of a feature to a specific range (usually between 0 and 1).\n",
    "\n",
    "Here is an example of the Unit Vector technique in action:\n",
    "\n",
    "Suppose we have a dataset with a feature that represents the height of individuals in centimeters. The height feature has values ranging from 150 to 200 cm. We want to scale the feature using the Unit Vector technique.\n",
    "\n",
    "To do this, we first calculate the L2 norm of the feature:\n",
    "\n",
    "||height|| = sqrt(150^2 + 160^2 + 170^2 + 180^2 + 190^2 + 200^2) = 671.32\n",
    "\n",
    "Then, we divide each value in the height feature by the L2 norm:\n",
    "\n",
    "height_scaled = height / 671.32\n",
    "\n",
    "For example, if an individual is 170 cm tall, the scaled height would be:\n",
    "height_scaled = 170 / 671.32 = 0.253\n",
    "\n",
    "This means that the scaled height value for this individual would be 0.253, which has a unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335da15b-73b3-4356-a6f5-c3f6f954d376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6f85d50-3dfe-4148-b32a-38a489a7784d",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an  example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e81773-a95b-43c2-a133-bddfca83812a",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "Principal Component Analysis (PCA) is a popular technique for dimensionality reduction that involves transforming high-dimensional data into a lower-dimensional space while retaining as much of the variance in the original data as possible.\n",
    "\n",
    "PCA is used in dimensionality reduction because it can reduce the number of features in the data while retaining most of the variance in the original data.\n",
    "\n",
    "Here is an example of PCA in action:\n",
    "Suppose we have a dataset with 1000 samples and 20 features. We want to reduce the dimensionality of the data while retaining as much information as possible.\n",
    "\n",
    "To do this, we can apply PCA to the data. First, we standardize the data by subtracting the mean and dividing by the standard deviation. Then, we compute the covariance matrix of the standardized data. Next, we compute the eigenvectors and eigenvalues of the covariance matrix. Finally, we choose the top k eigenvectors with the largest eigenvalues as the principal components and project the data onto the new space spanned by these eigenvectors.\n",
    "\n",
    "For example, if we choose k=5, we would have reduced the dimensionality of the data from 20 features to 5 principal components. We can then use these 5 principal components as the new features in our machine learning model. By using PCA to reduce the dimensionality of the data, we have reduced the computational complexity of our model and reduced the risk of overfitting, while retaining most of the information in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67182dd0-eb25-4ffb-b956-4ec45ac54216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e29e33d-2df3-4cba-97d1-1278aa758b91",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature  Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9989aa39-16b5-46c8-84e0-f6aa7f95373a",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "PCA and feature extraction are related in that PCA can be used as a feature extraction technique. PCA identifies the principal components of the data, which are linear combinations of the original features that capture the most significant variation in the data. By choosing the top k principal components, we can extract a new set of features that represent the most important information in the original data.\n",
    "\n",
    "Here is an example of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with 1000 samples and 20 features. We want to extract a new set of features that represent the most significant information in the original data.\n",
    "\n",
    "To do this, we can apply PCA to the data. First, we standardize the data by subtracting the mean and dividing by the standard deviation. Then, we compute the covariance matrix of the standardized data. Next, we compute the eigenvectors and eigenvalues of the covariance matrix. Finally, we choose the top k eigenvectors with the largest eigenvalues as the principal components and project the data onto the new space spanned by these eigenvectors.\n",
    "\n",
    "For example, if we choose k=5, we would have extracted 5 new features that represent the most significant information in the original data. We can then use these 5 new features as the input to our machine learning model. By using PCA for feature extraction, we have reduced the dimensionality of the data and extracted a new set of features that represent the most important information in the original data, which can help to improve the performance of our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c8987-f2ed-42b4-b03c-c2647499e441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8e572e2-f448-4c28-b036-708decc66488",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset  contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to  preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73b4de-99eb-435a-9a3a-94b53edc1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:\n",
    "    \n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data as follows:\n",
    "\n",
    "1. First, we would identify the numerical features in the dataset, such as price, rating, and delivery time.\n",
    "2. Next, we would apply Min-Max scaling to each of these features. Min-Max scaling rescales the data to a fixed range, typically [0,1].\n",
    "3. After applying Min-Max scaling, each feature would have values between 0 and 1, with 0 corresponding to the minimum value and 1 corresponding to the maximum value. This rescaling does not change the relationships between the data points, but it does change their absolute values.\n",
    "4. We can then use the Min-Max scaled features as input to our recommendation algorithm. For example, we can use a collaborative filtering algorithm that compares the similarity between users or items based on the Min-Max scaled features to generate personalized recommendations for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa18a4-fb7e-4a10-8f26-09d65f3f56df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b989884-4f3c-484a-b73d-7e13b508982e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many  features, such as company financial data and market trends. Explain how you would use PCA to reduce the  dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee46b10-8a9f-46c2-82b7-8ad1c0cdd876",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "In the context of building a model to predict stock prices, PCA can be used to reduce the dimensionality of the dataset as follows:\n",
    "1. First, we would identify the numerical features in the dataset, such as company financial data and market trends.\n",
    "2. Next, we would standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "3. After standardizing the data, we would compute the covariance matrix of the standardized data. The covariance matrix describes the relationships between the features and is used to identify the principal components of the data.\n",
    "4. We would then compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "5. We would choose the top k eigenvectors with the largest eigenvalues as the principal components. These principal components are linear combinations of the original features that capture the most significant variation in the data.\n",
    "6. We can then project the data onto the new space spanned by the selected principal components. The projected data has reduced dimensionality, with each observation represented by a new set of features that capture the most important information in the original data.\n",
    "7. Finally, we can use the reduced dimensionality data as input to our stock price prediction model. By using PCA to reduce the dimensionality of the data, we can reduce the number of features in the dataset and avoid overfitting. This can lead to more accurate predictions and improve the overall performance of our stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15304ed-bb0a-4c15-998b-4fb6ebbe7ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79cbba70-de00-4d81-807a-614853345da5",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the  values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57820f1-8a59-49f7-9a31-8e8f880ebb13",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "To perform Min-Max scaling to transform the values [1, 5, 10, 15, 20] to a range of -1 to 1, we can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d237c20-8c29-4a7a-a786-a44cabacde2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.579, -0.05259999999999998, 0.4736, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 1. Find the minimum and maximum values of the dataset:\n",
    "min_value = 1\n",
    "max_value = 20\n",
    "\n",
    "#2. Compute the range of the dataset\n",
    "range = max_value - min_value \n",
    "\n",
    "#3. For each value in the dataset, subtract the minimum value and divide by the range:\n",
    "scaled_values = [(x - min_value) / range for x in [1, 5, 10, 15, 20]]\n",
    "scaled_values = [(1-1)/19, (5-1)/19, (10-1)/19, (15-1)/19, (20-1)/19]\n",
    "scaled_values = [0.0000, 0.2105, 0.4737, 0.7368, 1.0000]\n",
    "\n",
    "#4. Rescale the values to the range of -1 to 1:\n",
    "scaled_values = [(x * 2) - 1 for x in scaled_values]\n",
    "print(scaled_values)\n",
    "\n",
    "#Therefore, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are [-1.0000, -0.5789, 0.0525, 0.4737, 1.0000]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915870fa-e95f-4a09-ae83-c43e049222d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20b13cec-d9cb-46b5-9b9b-d53991d863f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d0aa6-3b4d-4dd1-87b3-c59c426d4406",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "The number of principal components to retain in PCA depends on the specific characteristics of the dataset, the amount of variance we want to retain, and the trade-off between dimensionality reduction and information loss.\n",
    "\n",
    "To determine the optimal number of principal components to retain, we can use a scree plot or cumulative explained variance plot to visualize the percentage of total variance explained by each principal component. We can then choose the number of principal components that explain a sufficiently high percentage of the variance, while also keeping the dimensionality of the dataset low.\n",
    "\n",
    "In this case, without more information about the dataset, it's difficult to determine the optimal number of principal components to retain. However, we can still provide a general guideline. A common rule of thumb is to retain enough principal components to explain at least 70-80% of the variance in the data. This would ensure that we capture most of the important information in the dataset while also reducing its dimensionality.\n",
    "\n",
    "For example, if after applying PCA we find that the first two principal components explain 75% of the total variance, we could choose to retain those two components and discard the remaining ones. This would result in a reduced dataset with only two features (the two principal components), which could be easier to visualize, analyze, and use as input to a predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8fca1-3299-45d7-a8bd-521db7d2701c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db82fed-e10e-4b79-8a2d-19102e9e4505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafbf9-f606-4139-bba8-8c513dd9fd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
