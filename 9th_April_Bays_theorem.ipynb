{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd746959-1ef9-432d-a721-fca8e87e8f57",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aefb78b-8041-468b-b0ac-afdb0bcc04b8",
   "metadata": {},
   "source": [
    "### Ans. :\n",
    "\n",
    "Bayes' theorem is a mathematical formula used to calculate the probability of an event based on prior knowledge or information. It is named after Reverend Thomas Bayes, an 18th-century British statistician and theologian who first formulated it.\n",
    "\n",
    "#### The theorem can be stated as follows:\n",
    "\n",
    "P(A|B) = P(B|A) x P(A) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the probability of event A occurring given that event B has occurred\n",
    "\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred\n",
    "P(A) is the prior probability of event A occurring before the new evidence (event B) is considered\n",
    "\n",
    "P(B) is the probability of event B occurring, which can be calculated as the sum of the probabilities of all possible ways in which event B can occur.\n",
    "Bayes' theorem is widely used in statistics, machine learning, and other fields to make predictions and decisions based on prior information and new evidence. It is particularly useful in situations where we have incomplete information and need to update our beliefs as new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccbf163-4b1a-4f6d-9ec5-5f24dc3fd7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ed3d45-5446-4520-b229-cafc7df3e647",
   "metadata": {},
   "source": [
    "## Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5039a4e-849e-425f-969b-f09bf6288f90",
   "metadata": {},
   "source": [
    "### Ans. :\n",
    "\n",
    "The formula for Bayes' theorem is:\n",
    "\n",
    "#### P(A|B) = P(B|A) x P(A) / P(B)\n",
    "Where:\n",
    "\n",
    "P(A|B) is the probability of event A occurring given that event B has occurred\n",
    "\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred\n",
    "\n",
    "P(A) is the prior probability of event A occurring before the new evidence (event B) is considered\n",
    "\n",
    "P(B) is the probability of event B occurring, which can be calculated as the sum of the probabilities of all possible ways in which event B can occur.\n",
    "\n",
    "This formula allows us to update our prior beliefs about the probability of an event occurring in light of new evidence. By multiplying the prior probability by the likelihood of the evidence given the event, and dividing by the overall probability of the evidence, we get the posterior probability of the event given the new evidence. Bayes' theorem is a powerful tool for decision-making, prediction, and inference in many fields, including statistics, machine learning, and artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c18aa5-278c-46a0-835b-2d428375891c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582d408e-7e91-4a07-9655-c552461a5506",
   "metadata": {},
   "source": [
    "## Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d772c7-7963-40fb-a25a-1b5ee31270d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans. :\n",
    "Bayes' theorem is used in many practical applications, including:\n",
    "Medical diagnosis: Bayes' theorem is used to calculate the probability of a patient having a disease given the results of a medical test. The prior probability is the prevalence of the disease in the population, and the likelihood is the probability of a positive test result given the patient has the disease.\n",
    "\n",
    "1. Spam filtering: Bayes' theorem is used to classify emails as spam or non-spam. The prior probability is the overall proportion of spam and non-spam emails, and the likelihood is the probability of certain words or phrases occurring in spam emails.\n",
    "\n",
    "2. Machine learning: Bayes' theorem is used in many machine learning algorithms, including Naive Bayes classifiers, which are widely used for text classification, sentiment analysis, and recommendation systems.\n",
    "\n",
    "3. Weather forecasting: Bayes' theorem can be used to update weather forecasts based on new data, such as satellite imagery or radar data. The prior probability is the forecast based on previous data, and the likelihood is the probability of the new data given the forecast.\n",
    "4. Fraud detection: Bayes' theorem is used to detect fraudulent transactions in credit card data. The prior probability is the overall proportion of fraudulent and non-fraudulent transactions, and the likelihood is the probability of certain patterns of transactions occurring in fraudulent cases.\n",
    "\n",
    "Overall, Bayes' theorem is a powerful tool for updating beliefs and making decisions based on prior knowledge and new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7db6f-e9f4-4c5e-8c2c-19c333c4f785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bb6dc59-326b-4aab-8900-09998b0cec3d",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d95e2-ee52-4709-840e-9c13221def81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans. :\n",
    "Bayes' theorem and conditional probability are related concepts, but they are used in different ways. Conditional probability is used to calculate the probability of an event given that another event has occurred. Bayes' theorem, on the other hand, is used to update the probability of an event based on new evidence.\n",
    "\n",
    "In Bayes' theorem, conditional probability is used as one of the inputs to calculate the posterior probability. Specifically, the likelihood term in the formula is the conditional probability of the new evidence given the event of interest. The prior probability in the formula represents the probability of the event before the new evidence is considered. By multiplying the prior probability by the likelihood and dividing by the overall probability of the new evidence, we get the posterior probability of the event given the new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb190e6-db29-491a-abfc-88a9c2a4eea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ff07c5-4557-4e0e-beab-0649539086ab",
   "metadata": {},
   "source": [
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a433f7-dbc9-4bc5-9c2a-57b88f8b96bf",
   "metadata": {},
   "source": [
    "### Ans. :\n",
    "#### There are three types of Naive Bayes classifiers commonly used in practice:\n",
    "1. Gaussian Naive Bayes: This classifier is used when the features are continuous and assumed to follow a Gaussian (normal) distribution. It is a good choice when the features are normally distributed or can be transformed to have a normal distribution.\n",
    "2. Multinomial Naive Bayes: This classifier is used when the features are discrete and represent counts or frequencies, such as word frequencies in a text document. It is commonly used for text classification, spam filtering, and sentiment analysis.\n",
    "3. Bernoulli Naive Bayes: This classifier is used when the features are binary, such as the presence or absence of certain words in a text document. It is a good choice when the focus is on the presence or absence of features rather than their frequency.\n",
    "To choose which type of Naive Bayes classifier to use for a given problem, you should consider the type of features in your data and the assumptions made by each classifier. If the features are continuous and assumed to follow a normal distribution, Gaussian Naive Bayes may be appropriate. If the features are discrete and represent counts or frequencies, Multinomial Naive Bayes may be appropriate. If the features are binary, Bernoulli Naive Bayes may be appropriate.\n",
    "In practice, it is common to try multiple types of Naive Bayes classifiers and compare their performance on a validation set to choose the best one for a given problem. It is also important to preprocess the data appropriately and handle missing values, outliers, and imbalances in the data, which can affect the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded58878-0ec0-4985-a25c-c66140d4c923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7827ab8-39b1-42d4-95f8-ba9d946a5a74",
   "metadata": {},
   "source": [
    "## Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A       3    3   4    4    3    3    3\n",
    "B       2    2   1    2    2    2    3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc502b4-0753-4c1a-af9f-d1cbf12efa0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ans. :\n",
    "To predict the class of a new instance with features X1=3 and X2=4 using Naive Bayes, we need to calculate the posterior probability of each class given the features, and then choose the class with the highest probability.\n",
    "\n",
    "#### Using the Naive Bayes formula:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)\n",
    "\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)\n",
    "\n",
    "Since we are assuming equal prior probabilities for each class, P(A) = P(B) = 0.5.\n",
    "\n",
    "#### To calculate the likelihood term, we can use the frequency table:\n",
    "\n",
    "P(X1=3,X2=4|A) = 3/16 * 3/16 = 9/256\n",
    "\n",
    "P(X1=3,X2=4|B) = 1/9 * 3/9 = 1/27\n",
    "\n",
    "#### To calculate the marginal probability of the features, we can sum over all possible class values:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B) = 9/256 * 0.5 + 1/27 * 0.5 = 0.0265\n",
    "\n",
    "#### Finally, we can calculate the posterior probabilities:\n",
    "\n",
    "P(A|X1=3,X2=4) = 9/256 * 0.5 / 0.0265 ≈ 0.858\n",
    "\n",
    "P(B|X1=3,X2=4) = 1/27 * 0.5 / 0.0265 ≈ 0.142\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class A, as it has a higher posterior probability of approximately 0.858 compared to class B's probability of approximately 0.142."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d87ea-b8e0-4f4b-90b4-81ada6407c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
