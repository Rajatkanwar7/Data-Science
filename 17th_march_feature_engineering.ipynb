{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4147d6b-e987-4c9e-a523-cfaa0092d867",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name somealgorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd54a14-0ff7-4337-a5e9-47031d4837d2",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "Missing values in a dataset are entries or observations that have no data or information recorded for one or more features. These missing values can occur due to a variety of reasons, such as data collection errors, incomplete surveys, or data corruption during storage.\n",
    "\n",
    "Handling missing values is essential in data analysis because these values can affect the accuracy, reliability, and validity of the results obtained from the dataset. If missing values are not handled properly, they can lead to biased or erroneous conclusions, and can also affect the performance of machine learning algorithms.\n",
    "\n",
    "Some of the common algorithms that are not affected by missing values are decision trees, Random Forest, and AdaBoost. These algorithms can handle missing values by ignoring them during the split criteria. Additionally, k-Nearest Neighbor (KNN) and Support Vector Machines (SVM) can also handle missing values by using imputation techniques to fill in missing values before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3772d7-a74f-4406-b7f1-6764358189aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09cd968e-df1c-4400-9b90-4fa9d9a7081f",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0614a05-87d4-402c-80ea-8862b5985bd1",
   "metadata": {},
   "source": [
    "Ans:\n",
    "There are several techniques used to handle missing data, some of which are:\n",
    "\n",
    "### 1. Deletion: \n",
    "This involves removing the missing values from the dataset. This technique is only used when the missing values are very few and does not affect the overall analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491a55a2-39ec-4df9-9120-5602ccfa75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatFrame before dropping null value:\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  NaN   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n",
      "\n",
      "\n",
      "DatFrame after dropping null value:\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df1= df.dropna()\n",
    "\n",
    "print(\"DatFrame before dropping null value:\\n\",df)\n",
    "print(\"\\n\")\n",
    "print(\"DatFrame after dropping null value:\\n\",df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9abac67-b7c5-45cc-b7af-76e9175e34dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd5a194-d1c1-454a-9095-aae673ffca3b",
   "metadata": {},
   "source": [
    "### 2. Mean/Mode/Median Imputation: \n",
    "This involves replacing the missing values with the mean/mode/median of the available values. This technique is useful when the number of missing values is small and the data is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b666c41-f581-4d70-8127-5b92f8881e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatFrame before Imputing missing values with mean :\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  NaN   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n",
      "\n",
      "\n",
      "DatFrame after Imputing missing values with mean:\n",
      "      A      B\n",
      "0  1.0   6.00\n",
      "1  2.0   8.25\n",
      "2  3.0   8.00\n",
      "3  4.0   9.00\n",
      "4  5.0  10.00\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Imputing missing values with mean\n",
    "df1= df.fillna(df.mean())\n",
    "\n",
    "print(\"DatFrame before Imputing missing values with mean :\\n\",df)\n",
    "print(\"\\n\")\n",
    "print(\"DatFrame after Imputing missing values with mean:\\n\",df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177252bc-e34d-464c-b30f-6821913729ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37729f90-23bb-4231-8da2-e77733efd63e",
   "metadata": {},
   "source": [
    "### 3. Forward/Backward filling: \n",
    "This involves replacing the missing values with the last/next observed value. This technique is useful when the data is time-series and the values do not change rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ccf71f-2c81-4a30-8b55-f6acea40e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatFrame before Filling missing values with last observation :\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  NaN   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n",
      "\n",
      "\n",
      "DatFrame after Filling missing values with last observation:\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   6.0\n",
      "2  2.0   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Filling missing values with last observation\n",
    "df1=df.fillna(method='ffill')\n",
    "print(\"DatFrame before Filling missing values with last observation :\\n\",df)\n",
    "print(\"\\n\")\n",
    "print(\"DatFrame after Filling missing values with last observation:\\n\",df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd393c-bb78-492c-b1aa-8471cef3fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d81e52-841a-4630-9f20-c6ef0b01ff42",
   "metadata": {},
   "source": [
    "### 4. Interpolation: \n",
    "This involves estimating missing values based on the values of other variables. This technique is useful when the data is time-series and values change rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be994717-0894-4d26-a8a6-92103e058ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatFrame before Interpolating missing values :\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  NaN   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n",
      "\n",
      "\n",
      "DatFrame after Interpolating missing values\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  3.0   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, np.nan, 8, 9, 10]})\n",
    "\n",
    "# Interpolating missing values\n",
    "df1=df.interpolate()\n",
    "\n",
    "print(\"DatFrame before Interpolating missing values :\\n\",df)\n",
    "print(\"\\n\")\n",
    "print(\"DatFrame after Interpolating missing values\\n\",df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb9312-0706-4fca-a427-400342f2b241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a43a077-dc40-4b4d-863b-41120e317cf9",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2ab0b-5cce-4f88-985c-d05e16827c4e",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "### Imbalanced data\n",
    "is a common problem in data analysis where the distribution of the target variable (or class labels) is not uniform, i.e., one class is represented by significantly fewer instances compared to the other class(es). For example, in a medical diagnosis dataset, the number of healthy patients may be much larger than the number of patients with a particular disease.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased and inaccurate results in the analysis. This is because most machine learning algorithms are designed to maximize overall accuracy, which can lead to a tendency to classify most instances as belonging to the majority class. As a result, the minority class may be completely overlooked, and the model's performance on it may be poor.\n",
    "\n",
    "For example, let's say we have a dataset with 1000 instances, out of which 90% belong to class A and 10% belong to class B. If we train a model on this dataset without addressing the class imbalance issue, the model may classify all instances as belonging to class A and still achieve a 90% accuracy. However, this model would be of no use in predicting the minority class, and the overall results would be highly misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01e12e-c5ff-4cce-a0e2-b38ecf4454f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07592ac-5e67-4ad5-99db-ac3da4b2499b",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa2ca9-014c-48f8-a970-08e56338efa5",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "1. Down-sampling involves reducing the size of the majority class by randomly removing instances from it, so that the size of the majority class is closer to the size of the minority class. This can be useful when the majority class is much larger than the minority class, and the model is biased towards it.\n",
    "\n",
    "2. Up-sampling involves increasing the size of the minority class by duplicating instances from it or generating new instances based on the existing ones. This can be useful when the minority class is much smaller than the majority class, and the model is not able to learn enough from it.\n",
    "\n",
    "For example, let's say we have a dataset with 1000 instances, out of which 900 instances belong to class A and 100 instances belong to class B. In this case, the dataset is highly imbalanced, and the model may not be able to learn enough from the minority class. We can use up-sampling to increase the size of the minority class by generating new instances based on the existing ones or duplicating the existing ones, so that the number of instances in class B becomes closer to the number of instances in class A.\n",
    "\n",
    "Conversely, let's say we have a dataset with 1000 instances, out of which 100 instances belong to class A and 900 instances belong to class B. In this case, the dataset is highly imbalanced, and the model may be biased towards the majority class. We can use down-sampling to reduce the size of the majority class by randomly removing instances from it, so that the number of instances in class A becomes closer to the number of instances in class B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021938fe-f347-41b8-a059-91a2694679dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a76fcfcf-eb98-4b2b-8544-0bd196f672e7",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc4b4e-70d2-454c-bb27-e7da1c677a51",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Data augmentation is a technique used in machine learning to increase the size of a dataset by generating new, modified versions of existing data points. \n",
    "\n",
    "### SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "is a specific data augmentation technique used to address the class imbalance problem, which occurs when the number of samples in each class is not balanced. SMOTE works by generating synthetic samples for the minority class by interpolating between the existing minority class samples. The algorithm selects a sample from the minority class and then finds its k-nearest neighbors. Synthetic samples are then generated by randomly selecting one of the k-nearest neighbors and interpolating between the two points. This process is repeated until the desired level of class balance is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb388540-1528-4c59-8f96-a4572b89f299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f06943-ee63-4d93-9765-f68a12f69592",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6a440-c3aa-4e55-8b4f-5d3132610faf",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "Outliers in a dataset are data points that are significantly different from other data points in the same dataset. Outliers can be caused by a variety of reasons, such as measurement errors, data entry errors, or the presence of rare events or extreme values.\n",
    "\n",
    "It is essential to handle outliers because they can significantly affect the statistical analysis of a dataset and the performance of machine learning models trained on the dataset. Outliers can distort the mean, variance, and correlation of a dataset, leading to inaccurate statistical inferences. In machine learning, outliers can bias the training of a model and lead to poor performance on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea38c7c-e1b2-42d8-ae10-1cb35a8f25ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c1f21f8-22ea-436f-a6cf-d8ff1cf95cb6",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888bf67-d82c-4dff-a5b7-0b62e2fa2130",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "There are several techniques that can be used to handle missing data:\n",
    "\n",
    "1. Deletion: This involves removing the rows or columns that contain missing data. There are two types of deletion: listwise deletion, where entire rows are deleted if they contain any missing values, and pairwise deletion, where only the missing values in a particular analysis are deleted. Deletion can lead to loss of information and bias in the analysis, especially if there is a large amount of missing data.\n",
    "\n",
    "2. Imputation: This involves replacing missing values with estimates or predictions based on the observed data. There are several imputation techniques, such as mean imputation, median imputation, mode imputation, and regression imputation, which use statistical models to estimate missing values. Imputation can be a useful technique if the amount of missing data is small, but it can also introduce bias if the imputation model is misspecified.\n",
    "\n",
    "3. Model-based methods: These methods involve using a statistical model that accounts for missing data, such as maximum likelihood estimation or Bayesian analysis. These methods can be more accurate than imputation and less biased than deletion, but they require more complex modeling and may be computationally intensive.\n",
    "\n",
    "4. Multiple imputation: This involves creating multiple imputed datasets using different imputation methods and combining the results to obtain a final estimate. This approach can reduce the bias and increase the accuracy of the analysis compared to single imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330c91a-dbfe-409c-a1d2-4ece46d1cd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f3e9f6-8c77-461c-a70f-b91f7d4a66a6",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ff771-9219-45f6-b305-1889c2b7c03e",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "When working with a large dataset and missing data, it is important to determine whether the missing data is missing at random (MAR) or missing not at random (MNAR) because this can affect the validity of the analysis and the selection of appropriate imputation techniques. Here are some strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "\n",
    "1. Visual inspection: One way to identify patterns in missing data is to visually inspect the data using scatterplots, histograms, or heatmaps. This can help identify any relationships between missing values and other variables in the dataset.\n",
    "\n",
    "2. Statistical tests: There are several statistical tests that can be used to assess whether the missing data is missing at random. One common test is the Little's test, which tests whether the missing data is related to observed data in the dataset.\n",
    "\n",
    "3. Imputation: Another way to assess whether the missing data is missing at random is to use imputation methods and compare the results with and without imputation. If the imputation results are similar to the observed data, it suggests that the missing data is missing at random.\n",
    "\n",
    "4. Expert knowledge: Expert knowledge of the data and the data collection process can provide insights into whether the missing data is missing at random or not. For example, if missing data occurs in a particular region or demographic group, this may suggest a non-random pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ba300-41e6-4a4d-9828-3495deb0b639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9765f666-27ae-44e6-8b9c-c5497dea2e16",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974165f-8af6-47ce-a46e-821862e911ed",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "When working with imbalanced datasets, where one class is significantly underrepresented compared to the other, it can be challenging to evaluate the performance of a machine learning model. Here are some strategies that can be used to evaluate the performance of a model on an imbalanced dataset:\n",
    "\n",
    "1. Use alternative performance metrics: Accuracy is not a reliable performance metric for imbalanced datasets because it can be misleading if the model simply predicts the majority class. Instead, alternative metrics such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) should be used to evaluate the performance of the model.\n",
    "\n",
    "2. Resampling techniques: One way to address class imbalance is to use resampling techniques such as oversampling or undersampling. Oversampling involves randomly duplicating minority class samples to balance the classes, while undersampling involves randomly deleting majority class samples. Care should be taken to avoid overfitting and to ensure that the test set is not contaminated by resampled data.\n",
    "\n",
    "3. Cost-sensitive learning: Another way to address class imbalance is to use cost-sensitive learning, where the model is penalized more for misclassifying minority class samples. This can be achieved by adjusting the class weights or using a customized loss function.\n",
    "\n",
    "4. Ensemble methods: Ensemble methods such as bagging, boosting, or stacking can be used to combine multiple models and improve the performance on imbalanced datasets.\n",
    "\n",
    "5. Data augmentation: Data augmentation techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can be used to generate synthetic minority class samples to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9700da-c880-4053-a343-41524b69ed7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e1879c-4d66-45c9-986a-16022acff044",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13ed59-8dbb-4c2e-91b1-2990982dbcb1",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "To balance an unbalanced dataset, where one class is overrepresented compared to the other, there are several methods that can be used to down-sample the majority class:\n",
    "\n",
    "1. Random under-sampling: In this method, a random subset of the majority class is selected to match the size of the minority class. The disadvantage of this method is that some potentially useful data may be lost.\n",
    "\n",
    "2. Cluster-based under-sampling: In this method, the majority class is divided into clusters, and a representative sample is selected from each cluster. This can preserve more information than random under-sampling.\n",
    "\n",
    "3. Tomek links: Tomek links are pairs of samples from different classes that are close to each other but have different labels. Removing the majority class sample from a Tomek link can help improve the separation between the classes.\n",
    "\n",
    "4. Edited nearest neighbors: In this method, the majority class samples that are misclassified by their k-nearest neighbors are removed from the dataset. This can be an effective method for removing noisy samples.\n",
    "\n",
    "5. Synthetic minority over-sampling technique (SMOTE): SMOTE can be used to generate synthetic minority class samples by interpolating between existing minority class samples. This can help address the class imbalance problem and improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5bbdd-da61-4aa9-ab5c-b93cba4fd942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08cc7158-6c5f-4084-b45f-22de67214c04",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c077a5b-fc51-426e-acd8-b98c7158e032",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    \n",
    "To balance an unbalanced dataset with a low percentage of occurrences, where one class is underrepresented compared to the other, there are several methods that can be used to up-sample the minority class:\n",
    "\n",
    "1. Random over-sampling: In this method, random samples are drawn with replacement from the minority class until the size of the majority class is reached. This can lead to overfitting and should be used with caution.\n",
    "\n",
    "2. Synthetic minority over-sampling technique (SMOTE): SMOTE can be used to generate synthetic minority class samples by interpolating between existing minority class samples. This can help address the class imbalance problem and improve the performance of the model.\n",
    "\n",
    "3. Adaptive Synthetic Sampling (ADASYN): ADASYN is an extension of SMOTE that generates synthetic minority class samples by taking into account the density distribution of the minority class. This can help overcome the limitations of SMOTE in handling overlapping and non-linear data distributions.\n",
    "\n",
    "4. One-Class Classification (OCC): In this method, only the minority class is used to train the model, and the majority class is treated as an outlier. This can be effective when the majority class is significantly different from the minority class.\n",
    "\n",
    "5. Cost-sensitive learning: Cost-sensitive learning involves assigning different misclassification costs to different classes. In this case, the misclassification cost for the minority class can be set higher than the majority class to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73bfb5-42f6-43c6-be19-70a418b94477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
